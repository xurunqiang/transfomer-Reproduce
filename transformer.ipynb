{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "先导入需要的库函数",
   "id": "d72f8c9048c94461"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.498579Z",
     "start_time": "2025-10-30T02:51:34.483216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, List, Tuple\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "480cdfae8f30e2aa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "此处,用于验证CUDA是否可用",
   "id": "7b66184cd76709a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.530415Z",
     "start_time": "2025-10-30T02:51:34.512570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 验证包能否 import\n",
    "try:\n",
    "    import torch\n",
    "    print(\"✅ torch 导入成功，版本：\", torch.__version__)\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"❌ 未安装 torch\") from e\n",
    "\n",
    "# 2. 验证 CUDA 是否编译进 PyTorch\n",
    "print(\"CUDA 是否被编译进 PyTorch：\", torch.cuda.is_available())\n",
    "\n",
    "# 3. 验证 GPU 设备能否被 PyTorch 识别 & 创建张量\n",
    "if torch.cuda.is_available():\n",
    "    gpu_id = torch.cuda.current_device()\n",
    "    print(\"当前可见 GPU 编号：\", gpu_id)\n",
    "    print(\"GPU 名称：\", torch.cuda.get_device_name(gpu_id))\n",
    "    # 在 GPU 上随机生成 2×2 张量\n",
    "    x = torch.randn(2, 2, device=f\"cuda:{gpu_id}\")\n",
    "    print(\"张量 x 的设备：\", x.device, \"\\n张量值：\\n\", x)\n",
    "else:\n",
    "    print(\"⚠️  PyTorch 未检测到 GPU，已自动退回 CPU\")\n",
    "    x = torch.randn(2, 2)\n",
    "    print(\"张量 x 的设备：\", x.device)"
   ],
   "id": "845b25cc19135884",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ torch 导入成功，版本： 2.4.1+cpu\n",
      "CUDA 是否被编译进 PyTorch： False\n",
      "⚠️  PyTorch 未检测到 GPU，已自动退回 CPU\n",
      "张量 x 的设备： cpu\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1.Embeding",
   "id": "c480314ba1d15d6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.592541Z",
     "start_time": "2025-10-30T02:51:34.556827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. 定义原始英文句子变量（保留所有翻译结果）\n",
    "# ---------------------------\n",
    "english_sentences: List[str] = [\n",
    "    \"Early morning sunlight filters through the window and falls on the desk.\",\n",
    "    \"Xiao Ming carries his schoolbag and goes to school happily.\",\n",
    "    \"Mom is carefully making breakfast in the kitchen.\",\n",
    "    \"The flowers in the park are blooming extraordinarily brightly.\",\n",
    "    \"The little dog is running around cheerfully on the grass.\",\n",
    "    \"Dad takes me to the library to read books on weekends.\",\n",
    "    \"The stars at night hang in the sky like countless diamonds.\",\n",
    "    \"The teacher patiently explains difficult problems to the students.\",\n",
    "    \"My younger sister likes to hold her teddy bear while sleeping.\",\n",
    "    \"Autumn has arrived, and the leaves on the trees are slowly turning yellow.\",\n",
    "    \"The spring birds are very lively and fly everywhere.\"\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 文本预处理与分词（基础分词，适配Embedding输入）\n",
    "# ---------------------------\n",
    "def preprocess_and_tokenize(sentences: List[str]) -> List[List[str]]:\n",
    "    \"\"\"预处理：小写、去除标点、分词\"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sent in sentences:\n",
    "        # 小写转换+去除标点\n",
    "        cleaned = sent.lower().replace(\".\", \"\").replace(\",\", \"\").replace(\";\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "        # 空格分词（基础分词，复杂场景可替换为NLTK/Spacy分词器）\n",
    "        tokens = cleaned.split()\n",
    "        tokenized_sentences.append(tokens)\n",
    "    return tokenized_sentences\n",
    "\n",
    "# 执行分词，得到token列表（变量保留）\n",
    "tokenized_data: List[List[str]] = preprocess_and_tokenize(english_sentences)\n",
    "print(\"分词结果：\", tokenized_data)\n",
    "# ---------------------------\n",
    "# 3. 构建词汇表（变量保留，支持OOV和特殊token）\n",
    "# ---------------------------\n",
    "def build_vocab(tokenized_data: List[List[str]], min_freq: int = 1) -> Tuple[dict, dict]:\n",
    "    \"\"\"构建词→索引、索引→词的映射\"\"\"\n",
    "    # 统计词频\n",
    "    word_counter = Counter([token for sent in tokenized_data for token in sent])\n",
    "    # 过滤低频词，加入特殊token（PAD:填充, UNK:未登录词）\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<UNK>\": 1\n",
    "    }\n",
    "    # 词频≥min_freq的词加入词汇表\n",
    "    for word, freq in word_counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "    # 反向词汇表（索引→词）\n",
    "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "# 构建词汇表（变量保留）\n",
    "vocab: dict = build_vocab(tokenized_data)[0]  # 词→索引\n",
    "print(\"\\n词->索引：\", vocab)\n",
    "inv_vocab: dict = build_vocab(tokenized_data)[1]  # 索引→词\n",
    "print(\"\\n索引->词：\", inv_vocab)\n",
    "vocab_size: int = len(vocab)  # 词汇表大小（变量保留）\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 整数编码（将token转换为索引，变量保留）\n",
    "# ---------------------------\n",
    "def encode_tokens(tokenized_data: List[List[str]], vocab: dict) -> List[torch.Tensor]:\n",
    "    \"\"\"将句子token转换为整数索引序列\"\"\"\n",
    "    encoded_sentences = []\n",
    "    for tokens in tokenized_data:\n",
    "        # 未登录词映射为<UNK>（索引1）\n",
    "        encoded = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        encoded_sentences.append(torch.tensor(encoded, dtype=torch.long))\n",
    "    return encoded_sentences\n",
    "\n",
    "# 执行编码（变量保留）\n",
    "encoded_data: List[torch.Tensor] = encode_tokens(tokenized_data, vocab)\n",
    "print(\"\\n编码结果:\", encoded_data)\n",
    "# ---------------------------\n",
    "# 5. 序列填充（统一长度，变量保留）\n",
    "# ---------------------------\n",
    "def pad_sequences(encoded_data: List[torch.Tensor], pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"填充序列到最大长度，返回(batch_size, max_seq_len)\"\"\"\n",
    "    max_seq_len = max([len(seq) for seq in encoded_data])\n",
    "    padded_batch = torch.full((len(encoded_data), max_seq_len), pad_idx, dtype=torch.long)\n",
    "    for i, seq in enumerate(encoded_data):\n",
    "        padded_batch[i, :len(seq)] = seq\n",
    "    return padded_batch\n",
    "\n",
    "# 执行填充（变量保留，shape: [171, max_seq_len]）\n",
    "padded_input: torch.Tensor = pad_sequences(encoded_data, vocab[\"<PAD>\"])\n",
    "print(\"\\n为保证等长，对句子进行统一填充:\\n\", padded_input)\n",
    "# ---------------------------\n",
    "# 6. Embedding层定义与嵌入操作（核心步骤，变量保留）\n",
    "# ---------------------------\n",
    "# 超参数（变量保留，可调整）\n",
    "english_embedding_dim: int = 64  # Embedding向量维度（与Transformer的d_model对应）\n",
    "\n",
    "# 定义Embedding层（变量保留，可训练）\n",
    "embedding_layer: nn.Embedding = nn.Embedding(\n",
    "    num_embeddings=vocab_size,  # 词汇表大小\n",
    "    embedding_dim=english_embedding_dim,  # 嵌入维度\n",
    "    padding_idx=vocab[\"<PAD>\"]  # PAD token的嵌入设为0且不训练\n",
    ")\n",
    "\n",
    "# 执行Embedding操作（变量保留最终嵌入结果）\n",
    "english_embedded_output: torch.Tensor = embedding_layer(padded_input)\n",
    "\n",
    "# ---------------------------\n",
    "# 关键变量输出（验证结果）\n",
    "# ---------------------------\n",
    "print(f\"总结：\")\n",
    "print(f\"词汇表大小: {vocab_size}\")\n",
    "print(f\"填充后输入形状: {padded_input.shape}\")  # (句子数量, 最大序列长度)\n",
    "print(f\"Embedding输出形状: {english_embedded_output.shape}\")  # (句子数量, 最大序列长度, 嵌入维度)\n",
    "print(f\"示例：'sunlight'的索引: {vocab.get('sunlight', vocab['<UNK>'])}\")\n",
    "# 取出第一个句子前 2 个 token 的索引，再反查词汇表\n",
    "first3_idx = padded_input[0, :2].tolist()          # [idx1, idx2, idx3]\n",
    "first3_words = [inv_vocab[i] for i in first3_idx]  # [word1, word2, word3]\n",
    "\n",
    "print(f\"示例：第一个句子前 2 个 token 对应的词: {first3_words}\")\n",
    "print(f\"示例：第一个句子的嵌入向量前 2 个 token:\\n{english_embedded_output[0, :2, :10]}\")\n"
   ],
   "id": "45c939d23f71323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果： [['early', 'morning', 'sunlight', 'filters', 'through', 'the', 'window', 'and', 'falls', 'on', 'the', 'desk'], ['xiao', 'ming', 'carries', 'his', 'schoolbag', 'and', 'goes', 'to', 'school', 'happily'], ['mom', 'is', 'carefully', 'making', 'breakfast', 'in', 'the', 'kitchen'], ['the', 'flowers', 'in', 'the', 'park', 'are', 'blooming', 'extraordinarily', 'brightly'], ['the', 'little', 'dog', 'is', 'running', 'around', 'cheerfully', 'on', 'the', 'grass'], ['dad', 'takes', 'me', 'to', 'the', 'library', 'to', 'read', 'books', 'on', 'weekends'], ['the', 'stars', 'at', 'night', 'hang', 'in', 'the', 'sky', 'like', 'countless', 'diamonds'], ['the', 'teacher', 'patiently', 'explains', 'difficult', 'problems', 'to', 'the', 'students'], ['my', 'younger', 'sister', 'likes', 'to', 'hold', 'her', 'teddy', 'bear', 'while', 'sleeping'], ['autumn', 'has', 'arrived', 'and', 'the', 'leaves', 'on', 'the', 'trees', 'are', 'slowly', 'turning', 'yellow'], ['the', 'spring', 'birds', 'are', 'very', 'lively', 'and', 'fly', 'everywhere']]\n",
      "\n",
      "词->索引： {'<PAD>': 0, '<UNK>': 1, 'early': 2, 'morning': 3, 'sunlight': 4, 'filters': 5, 'through': 6, 'the': 7, 'window': 8, 'and': 9, 'falls': 10, 'on': 11, 'desk': 12, 'xiao': 13, 'ming': 14, 'carries': 15, 'his': 16, 'schoolbag': 17, 'goes': 18, 'to': 19, 'school': 20, 'happily': 21, 'mom': 22, 'is': 23, 'carefully': 24, 'making': 25, 'breakfast': 26, 'in': 27, 'kitchen': 28, 'flowers': 29, 'park': 30, 'are': 31, 'blooming': 32, 'extraordinarily': 33, 'brightly': 34, 'little': 35, 'dog': 36, 'running': 37, 'around': 38, 'cheerfully': 39, 'grass': 40, 'dad': 41, 'takes': 42, 'me': 43, 'library': 44, 'read': 45, 'books': 46, 'weekends': 47, 'stars': 48, 'at': 49, 'night': 50, 'hang': 51, 'sky': 52, 'like': 53, 'countless': 54, 'diamonds': 55, 'teacher': 56, 'patiently': 57, 'explains': 58, 'difficult': 59, 'problems': 60, 'students': 61, 'my': 62, 'younger': 63, 'sister': 64, 'likes': 65, 'hold': 66, 'her': 67, 'teddy': 68, 'bear': 69, 'while': 70, 'sleeping': 71, 'autumn': 72, 'has': 73, 'arrived': 74, 'leaves': 75, 'trees': 76, 'slowly': 77, 'turning': 78, 'yellow': 79, 'spring': 80, 'birds': 81, 'very': 82, 'lively': 83, 'fly': 84, 'everywhere': 85}\n",
      "\n",
      "索引->词： {0: '<PAD>', 1: '<UNK>', 2: 'early', 3: 'morning', 4: 'sunlight', 5: 'filters', 6: 'through', 7: 'the', 8: 'window', 9: 'and', 10: 'falls', 11: 'on', 12: 'desk', 13: 'xiao', 14: 'ming', 15: 'carries', 16: 'his', 17: 'schoolbag', 18: 'goes', 19: 'to', 20: 'school', 21: 'happily', 22: 'mom', 23: 'is', 24: 'carefully', 25: 'making', 26: 'breakfast', 27: 'in', 28: 'kitchen', 29: 'flowers', 30: 'park', 31: 'are', 32: 'blooming', 33: 'extraordinarily', 34: 'brightly', 35: 'little', 36: 'dog', 37: 'running', 38: 'around', 39: 'cheerfully', 40: 'grass', 41: 'dad', 42: 'takes', 43: 'me', 44: 'library', 45: 'read', 46: 'books', 47: 'weekends', 48: 'stars', 49: 'at', 50: 'night', 51: 'hang', 52: 'sky', 53: 'like', 54: 'countless', 55: 'diamonds', 56: 'teacher', 57: 'patiently', 58: 'explains', 59: 'difficult', 60: 'problems', 61: 'students', 62: 'my', 63: 'younger', 64: 'sister', 65: 'likes', 66: 'hold', 67: 'her', 68: 'teddy', 69: 'bear', 70: 'while', 71: 'sleeping', 72: 'autumn', 73: 'has', 74: 'arrived', 75: 'leaves', 76: 'trees', 77: 'slowly', 78: 'turning', 79: 'yellow', 80: 'spring', 81: 'birds', 82: 'very', 83: 'lively', 84: 'fly', 85: 'everywhere'}\n",
      "\n",
      "编码结果: [tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  7, 12]), tensor([13, 14, 15, 16, 17,  9, 18, 19, 20, 21]), tensor([22, 23, 24, 25, 26, 27,  7, 28]), tensor([ 7, 29, 27,  7, 30, 31, 32, 33, 34]), tensor([ 7, 35, 36, 23, 37, 38, 39, 11,  7, 40]), tensor([41, 42, 43, 19,  7, 44, 19, 45, 46, 11, 47]), tensor([ 7, 48, 49, 50, 51, 27,  7, 52, 53, 54, 55]), tensor([ 7, 56, 57, 58, 59, 60, 19,  7, 61]), tensor([62, 63, 64, 65, 19, 66, 67, 68, 69, 70, 71]), tensor([72, 73, 74,  9,  7, 75, 11,  7, 76, 31, 77, 78, 79]), tensor([ 7, 80, 81, 31, 82, 83,  9, 84, 85])]\n",
      "\n",
      "为保证等长，对句子进行统一填充:\n",
      " tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  7, 12,  0],\n",
      "        [13, 14, 15, 16, 17,  9, 18, 19, 20, 21,  0,  0,  0],\n",
      "        [22, 23, 24, 25, 26, 27,  7, 28,  0,  0,  0,  0,  0],\n",
      "        [ 7, 29, 27,  7, 30, 31, 32, 33, 34,  0,  0,  0,  0],\n",
      "        [ 7, 35, 36, 23, 37, 38, 39, 11,  7, 40,  0,  0,  0],\n",
      "        [41, 42, 43, 19,  7, 44, 19, 45, 46, 11, 47,  0,  0],\n",
      "        [ 7, 48, 49, 50, 51, 27,  7, 52, 53, 54, 55,  0,  0],\n",
      "        [ 7, 56, 57, 58, 59, 60, 19,  7, 61,  0,  0,  0,  0],\n",
      "        [62, 63, 64, 65, 19, 66, 67, 68, 69, 70, 71,  0,  0],\n",
      "        [72, 73, 74,  9,  7, 75, 11,  7, 76, 31, 77, 78, 79],\n",
      "        [ 7, 80, 81, 31, 82, 83,  9, 84, 85,  0,  0,  0,  0]])\n",
      "总结：\n",
      "词汇表大小: 86\n",
      "填充后输入形状: torch.Size([11, 13])\n",
      "Embedding输出形状: torch.Size([11, 13, 64])\n",
      "示例：'sunlight'的索引: 4\n",
      "示例：第一个句子前 2 个 token 对应的词: ['early', 'morning']\n",
      "示例：第一个句子的嵌入向量前 2 个 token:\n",
      "tensor([[-1.9077e+00, -8.1809e-01,  1.3342e+00, -1.6592e+00, -2.0412e+00,\n",
      "         -4.2997e-01, -1.1602e+00,  3.1323e-01,  8.1319e-01, -9.5630e-02],\n",
      "        [ 2.0791e-01,  1.0443e+00,  2.0408e+00,  4.1197e-01,  8.3351e-04,\n",
      "          1.4102e-01,  1.7999e+00,  2.6963e-01, -9.6623e-01, -1.5682e+00]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.637752Z",
     "start_time": "2025-10-30T02:51:34.612723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# 1. 定义原始中文分词句子变量（保留所有句子）\n",
    "# ---------------------------\n",
    "chinese_tokenized_sentences: List[str] = [\n",
    "    \"清晨 阳光 透过 窗户 洒 在 书桌 上\",\n",
    "    \"小明 背着 书包 高高兴兴 地 去 学校\",\n",
    "    \"妈妈 在 厨房 里 认真 地 做 早餐\",\n",
    "    \"公园里 的 花儿 开 得 格外 鲜艳\",\n",
    "    \"小狗 欢快 地 在 草地上 跑 来 跑 去\",\n",
    "    \"爸爸 周末 带 我 去 图书馆 看书\",\n",
    "    \"夜晚 的 星星 像 无数 颗 钻石 挂 在 天空\",\n",
    "    \"老师 耐心 地 给 同学们 讲解 难题\",\n",
    "    \"妹妹 喜欢 抱着 她 的 玩具 熊 睡觉\",\n",
    "    \"秋天 到了 ， 树上 的 叶子 慢慢 变黄\",\n",
    "    \"春天 的 小鸟 很 活泼 ， 到处 飞\"\n",
    "]\n",
    "\n",
    "# 转换为token列表（按空格拆分，变量保留）\n",
    "chinese_tokens_list: List[List[str]] = [sent.split() for sent in chinese_tokenized_sentences]\n",
    "print(\"分词结果：\", chinese_tokens_list)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. 构建中文词汇表（变量保留，支持OOV和特殊token）\n",
    "# ---------------------------\n",
    "def build_chinese_vocab(tokenized_data: List[List[str]], min_freq: int = 1) -> Tuple[dict, dict]:\n",
    "    \"\"\"构建中文词→索引、索引→词的映射\"\"\"\n",
    "    word_counter = Counter([token for sent in tokenized_data for token in sent])\n",
    "    # 过滤标点，加入特殊token（PAD:填充, UNK:未登录词）\n",
    "    vocab = {\n",
    "        \"<PAD>\": 0,\n",
    "        \"<UNK>\": 1\n",
    "    }\n",
    "    for word, freq in word_counter.items():\n",
    "        if freq >= min_freq and word not in [\",\", \"，\"]:  # 过滤标点\n",
    "            vocab[word] = len(vocab)\n",
    "    inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "# 构建词汇表（变量保留）\n",
    "chinese_vocab: dict = build_chinese_vocab(chinese_tokens_list)[0]  # 词→索引\n",
    "print(\"\\n词->索引：\", chinese_vocab)\n",
    "chinese_inv_vocab: dict = build_chinese_vocab(chinese_tokens_list)[1]  # 索引→词\n",
    "print(\"\\n索引->词：\", chinese_inv_vocab)\n",
    "chinese_vocab_size: int = len(chinese_vocab)  # 词汇表大小（变量保留）\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 整数编码（将token转换为索引，变量保留）\n",
    "# ---------------------------\n",
    "def encode_chinese_tokens(tokenized_data: List[List[str]], vocab: dict) -> List[torch.Tensor]:\n",
    "    \"\"\"中文token转换为整数索引序列\"\"\"\n",
    "    encoded = []\n",
    "    for tokens in tokenized_data:\n",
    "        seq = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]  # OOV映射为UNK\n",
    "        encoded.append(torch.tensor(seq, dtype=torch.long))\n",
    "    return encoded\n",
    "\n",
    "# 执行编码（变量保留）\n",
    "chinese_encoded_data: List[torch.Tensor] = encode_chinese_tokens(chinese_tokens_list, chinese_vocab)\n",
    "print(\"\\n编码结果:\", chinese_encoded_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 序列填充（统一长度，变量保留）\n",
    "# ---------------------------\n",
    "def pad_chinese_sequences(encoded_data: List[torch.Tensor], pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"填充序列到最大长度，返回(batch_size, max_seq_len)\"\"\"\n",
    "    max_seq_len = max([len(seq) for seq in encoded_data])\n",
    "    padded_batch = torch.full((len(encoded_data), max_seq_len), pad_idx, dtype=torch.long)\n",
    "    for i, seq in enumerate(encoded_data):\n",
    "        padded_batch[i, :len(seq)] = seq\n",
    "    return padded_batch\n",
    "\n",
    "# 执行填充（变量保留）\n",
    "chinese_padded_input: torch.Tensor = pad_chinese_sequences(chinese_encoded_data, chinese_vocab[\"<PAD>\"])\n",
    "print(\"\\n为保证等长，对句子进行统一填充:\\n\", chinese_padded_input)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. 中文Embedding层定义与嵌入操作（核心步骤，变量保留）\n",
    "# ---------------------------\n",
    "# 超参数（变量保留，可调整）\n",
    "chinese_embedding_dim: int = 64  # 嵌入维度（与Transformer的d_model对应）\n",
    "\n",
    "# 定义Embedding层（变量保留，可训练）\n",
    "chinese_embedding_layer: nn.Embedding = nn.Embedding(\n",
    "    num_embeddings=chinese_vocab_size,\n",
    "    embedding_dim=chinese_embedding_dim,\n",
    "    padding_idx=chinese_vocab[\"<PAD>\"]  # PAD token的嵌入设为0且不训练\n",
    ")\n",
    "\n",
    "# 执行Embedding操作（变量保留最终嵌入结果）\n",
    "chinese_embedded_output: torch.Tensor = chinese_embedding_layer(chinese_padded_input)\n",
    "\n",
    "# ---------------------------\n",
    "# 关键变量输出（验证结果，与英文格式一致）\n",
    "# ---------------------------\n",
    "print(f\"\\n总结：\")\n",
    "print(f\"词汇表大小: {chinese_vocab_size}\")\n",
    "print(f\"填充后输入形状: {chinese_padded_input.shape}\")  # (句子数量, 最大序列长度)\n",
    "print(f\"Embedding输出形状: {chinese_embedded_output.shape}\")  # (句子数量, 最大序列长度, 嵌入维度)\n",
    "print(f\"示例：'阳光'的索引: {chinese_vocab.get('阳光', chinese_vocab['<UNK>'])}\")\n",
    "# 取出第一个句子前 2 个 token 的索引，再反查词汇表\n",
    "first3_idx = chinese_padded_input[0, :2].tolist()          # [idx1, idx2, idx3]\n",
    "first3_words = [chinese_inv_vocab[i] for i in first3_idx]  # [word1, word2, word3]\n",
    "\n",
    "print(f\"示例：第一个句子前 2 个 token 对应的词: {first3_words}\")\n",
    "print(f\"示例：第一个句子的嵌入向量前 2 个 token:\\n{chinese_embedded_output[0, :2, :10]}\")\n"
   ],
   "id": "1c47845a39f38dcc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果： [['清晨', '阳光', '透过', '窗户', '洒', '在', '书桌', '上'], ['小明', '背着', '书包', '高高兴兴', '地', '去', '学校'], ['妈妈', '在', '厨房', '里', '认真', '地', '做', '早餐'], ['公园里', '的', '花儿', '开', '得', '格外', '鲜艳'], ['小狗', '欢快', '地', '在', '草地上', '跑', '来', '跑', '去'], ['爸爸', '周末', '带', '我', '去', '图书馆', '看书'], ['夜晚', '的', '星星', '像', '无数', '颗', '钻石', '挂', '在', '天空'], ['老师', '耐心', '地', '给', '同学们', '讲解', '难题'], ['妹妹', '喜欢', '抱着', '她', '的', '玩具', '熊', '睡觉'], ['秋天', '到了', '，', '树上', '的', '叶子', '慢慢', '变黄'], ['春天', '的', '小鸟', '很', '活泼', '，', '到处', '飞']]\n",
      "\n",
      "词->索引： {'<PAD>': 0, '<UNK>': 1, '清晨': 2, '阳光': 3, '透过': 4, '窗户': 5, '洒': 6, '在': 7, '书桌': 8, '上': 9, '小明': 10, '背着': 11, '书包': 12, '高高兴兴': 13, '地': 14, '去': 15, '学校': 16, '妈妈': 17, '厨房': 18, '里': 19, '认真': 20, '做': 21, '早餐': 22, '公园里': 23, '的': 24, '花儿': 25, '开': 26, '得': 27, '格外': 28, '鲜艳': 29, '小狗': 30, '欢快': 31, '草地上': 32, '跑': 33, '来': 34, '爸爸': 35, '周末': 36, '带': 37, '我': 38, '图书馆': 39, '看书': 40, '夜晚': 41, '星星': 42, '像': 43, '无数': 44, '颗': 45, '钻石': 46, '挂': 47, '天空': 48, '老师': 49, '耐心': 50, '给': 51, '同学们': 52, '讲解': 53, '难题': 54, '妹妹': 55, '喜欢': 56, '抱着': 57, '她': 58, '玩具': 59, '熊': 60, '睡觉': 61, '秋天': 62, '到了': 63, '树上': 64, '叶子': 65, '慢慢': 66, '变黄': 67, '春天': 68, '小鸟': 69, '很': 70, '活泼': 71, '到处': 72, '飞': 73}\n",
      "\n",
      "索引->词： {0: '<PAD>', 1: '<UNK>', 2: '清晨', 3: '阳光', 4: '透过', 5: '窗户', 6: '洒', 7: '在', 8: '书桌', 9: '上', 10: '小明', 11: '背着', 12: '书包', 13: '高高兴兴', 14: '地', 15: '去', 16: '学校', 17: '妈妈', 18: '厨房', 19: '里', 20: '认真', 21: '做', 22: '早餐', 23: '公园里', 24: '的', 25: '花儿', 26: '开', 27: '得', 28: '格外', 29: '鲜艳', 30: '小狗', 31: '欢快', 32: '草地上', 33: '跑', 34: '来', 35: '爸爸', 36: '周末', 37: '带', 38: '我', 39: '图书馆', 40: '看书', 41: '夜晚', 42: '星星', 43: '像', 44: '无数', 45: '颗', 46: '钻石', 47: '挂', 48: '天空', 49: '老师', 50: '耐心', 51: '给', 52: '同学们', 53: '讲解', 54: '难题', 55: '妹妹', 56: '喜欢', 57: '抱着', 58: '她', 59: '玩具', 60: '熊', 61: '睡觉', 62: '秋天', 63: '到了', 64: '树上', 65: '叶子', 66: '慢慢', 67: '变黄', 68: '春天', 69: '小鸟', 70: '很', 71: '活泼', 72: '到处', 73: '飞'}\n",
      "\n",
      "编码结果: [tensor([2, 3, 4, 5, 6, 7, 8, 9]), tensor([10, 11, 12, 13, 14, 15, 16]), tensor([17,  7, 18, 19, 20, 14, 21, 22]), tensor([23, 24, 25, 26, 27, 28, 29]), tensor([30, 31, 14,  7, 32, 33, 34, 33, 15]), tensor([35, 36, 37, 38, 15, 39, 40]), tensor([41, 24, 42, 43, 44, 45, 46, 47,  7, 48]), tensor([49, 50, 14, 51, 52, 53, 54]), tensor([55, 56, 57, 58, 24, 59, 60, 61]), tensor([62, 63,  1, 64, 24, 65, 66, 67]), tensor([68, 24, 69, 70, 71,  1, 72, 73])]\n",
      "\n",
      "为保证等长，对句子进行统一填充:\n",
      " tensor([[ 2,  3,  4,  5,  6,  7,  8,  9,  0,  0],\n",
      "        [10, 11, 12, 13, 14, 15, 16,  0,  0,  0],\n",
      "        [17,  7, 18, 19, 20, 14, 21, 22,  0,  0],\n",
      "        [23, 24, 25, 26, 27, 28, 29,  0,  0,  0],\n",
      "        [30, 31, 14,  7, 32, 33, 34, 33, 15,  0],\n",
      "        [35, 36, 37, 38, 15, 39, 40,  0,  0,  0],\n",
      "        [41, 24, 42, 43, 44, 45, 46, 47,  7, 48],\n",
      "        [49, 50, 14, 51, 52, 53, 54,  0,  0,  0],\n",
      "        [55, 56, 57, 58, 24, 59, 60, 61,  0,  0],\n",
      "        [62, 63,  1, 64, 24, 65, 66, 67,  0,  0],\n",
      "        [68, 24, 69, 70, 71,  1, 72, 73,  0,  0]])\n",
      "\n",
      "总结：\n",
      "词汇表大小: 74\n",
      "填充后输入形状: torch.Size([11, 10])\n",
      "Embedding输出形状: torch.Size([11, 10, 64])\n",
      "示例：'阳光'的索引: 3\n",
      "示例：第一个句子前 2 个 token 对应的词: ['清晨', '阳光']\n",
      "示例：第一个句子的嵌入向量前 2 个 token:\n",
      "tensor([[-0.2951, -0.7905,  0.0197,  0.3987,  0.4551,  0.2550,  1.2473,  0.1035,\n",
      "         -1.2611,  0.2596],\n",
      "        [-0.0525,  1.3090, -0.1762, -0.6651,  1.2180, -1.4612, -0.7919, -1.1300,\n",
      "         -0.7296, -0.5140]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2、位置编码",
   "id": "8370b658bca4be4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.669864Z",
     "start_time": "2025-10-30T02:51:34.646755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1. Positional Encoding（位置编码）\n",
    "# ------------------------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "     sinusoid 位置编码，与《Attention is All You Need》原文一致\n",
    "    输出 shape 与 word embedding 相同，直接相加\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # 预先计算好 [max_len, d_model] 的编码矩阵，节省训练时 CPU->GPU 拷贝\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # [0,1,2,...,max_len-1]^T -> [max_len,1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # 分母项：10000^(2i/d_model) 取对数后一次性计算\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() *\n",
    "            (-math.log(10000.0) / d_model)          # 负号保证指数递减\n",
    "        )                                           # shape [d_model/2]\n",
    "\n",
    "        # 偶数列 sin，奇数列 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)   # [max_len, d_model/2]\n",
    "        if d_model % 2 == 1:                           # 奇数维最后一列单独处理\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)                           # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)                 # 不参与训练，持久化到 state_dict\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len, d_model]\n",
    "        # 截取与当前序列等长的一段位置编码，并加到 x 上\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x"
   ],
   "id": "79c7810e31262269",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.701595Z",
     "start_time": "2025-10-30T02:51:34.679551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 复用之前定义的PositionalEncoding类（无需重新定义）\n",
    "# 初始化位置编码层（与英文Embedding维度一致）\n",
    "english_pos_encoder = PositionalEncoding(d_model=english_embedding_dim)\n",
    "\n",
    "# 打印位置编码矩阵信息\n",
    "print(f\"英文位置编码矩阵形状：{english_pos_encoder.pe.shape}\")  # (1, 5000, 64)\n",
    "print(f\"前5个位置的前10维编码：\")\n",
    "print(english_pos_encoder.pe[0, :5, :10])\n",
    "\n",
    "# 对英文Embedding结果添加位置编码\n",
    "english_embedded_with_pos = english_pos_encoder(english_embedded_output)\n",
    "\n",
    "# 打印关键结果验证\n",
    "print(\"=\"*50)\n",
    "print(\"英文 Embedding + 位置编码 结果验证\")\n",
    "print(\"=\"*50)\n",
    "print(f\"原始Embedding形状：{english_embedded_output.shape}\")\n",
    "print(f\"添加位置编码后形状：{english_embedded_with_pos.shape}\")  # 形状保持一致\n",
    "print(f\"\\n第一个句子前2个token的最终表示（前10维）：\")\n",
    "print(\"token1（early）：\", english_embedded_with_pos[0, 0, :10])\n",
    "print(\"token2（morning）：\", english_embedded_with_pos[0, 1, :10])\n"
   ],
   "id": "d4c6bc3689296be4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文位置编码矩阵形状：torch.Size([1, 5000, 64])\n",
      "前5个位置的前10维编码：\n",
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.6816,  0.7318,  0.5332,  0.8460,  0.4093,  0.9124,\n",
      "          0.3110,  0.9504],\n",
      "        [ 0.9093, -0.4161,  0.9975,  0.0709,  0.9021,  0.4315,  0.7469,  0.6649,\n",
      "          0.5911,  0.8066],\n",
      "        [ 0.1411, -0.9900,  0.7783, -0.6279,  0.9933, -0.1160,  0.9536,  0.3010,\n",
      "          0.8126,  0.5828],\n",
      "        [-0.7568, -0.6536,  0.1415, -0.9899,  0.7785, -0.6277,  0.9933, -0.1157,\n",
      "          0.9536,  0.3011]])\n",
      "==================================================\n",
      "英文 Embedding + 位置编码 结果验证\n",
      "==================================================\n",
      "原始Embedding形状：torch.Size([11, 13, 64])\n",
      "添加位置编码后形状：torch.Size([11, 13, 64])\n",
      "\n",
      "第一个句子前2个token的最终表示（前10维）：\n",
      "token1（early）： tensor([-1.9077,  0.1819,  1.3342, -0.6592, -2.0412,  0.5700, -1.1602,  1.3132,\n",
      "         0.8132,  0.9044], grad_fn=<SliceBackward0>)\n",
      "token2（morning）： tensor([ 1.0494,  1.5846,  2.7223,  1.1437,  0.5340,  0.9870,  2.2092,  1.1820,\n",
      "        -0.6553, -0.6178], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.749097Z",
     "start_time": "2025-10-30T02:51:34.728063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 初始化位置编码层\n",
    "pos_encoder = PositionalEncoding(d_model=chinese_embedding_dim)\n",
    "# 打印位置编码矩阵信息\n",
    "print(f\"位置编码矩阵形状：{pos_encoder.pe.shape}\")  # (1, 5000, 64)\n",
    "print(f\"前5个位置的前10维编码：\")\n",
    "print(pos_encoder.pe[0, :5, :10])\n",
    "# 对Embedding结果添加位置编码\n",
    "embedded_with_pos = pos_encoder(chinese_embedded_output)\n",
    "\n",
    "# 打印关键结果验证\n",
    "print(\"=\"*50)\n",
    "print(\"Embedding + 位置编码 结果验证\")\n",
    "print(\"=\"*50)\n",
    "print(f\"原始Embedding形状：{chinese_embedded_output.shape}\")\n",
    "print(f\"添加位置编码后形状：{embedded_with_pos.shape}\")  # 形状保持一致\n",
    "print(f\"\\n第一个句子前2个token的最终表示（前10维）：\")\n",
    "print(\"token1（清晨）：\", embedded_with_pos[0, 0, :10])\n",
    "print(\"token2（阳光）：\", embedded_with_pos[0, 1, :10])\n"
   ],
   "id": "76e6e7d08d99b6a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "位置编码矩阵形状：torch.Size([1, 5000, 64])\n",
      "前5个位置的前10维编码：\n",
      "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  1.0000],\n",
      "        [ 0.8415,  0.5403,  0.6816,  0.7318,  0.5332,  0.8460,  0.4093,  0.9124,\n",
      "          0.3110,  0.9504],\n",
      "        [ 0.9093, -0.4161,  0.9975,  0.0709,  0.9021,  0.4315,  0.7469,  0.6649,\n",
      "          0.5911,  0.8066],\n",
      "        [ 0.1411, -0.9900,  0.7783, -0.6279,  0.9933, -0.1160,  0.9536,  0.3010,\n",
      "          0.8126,  0.5828],\n",
      "        [-0.7568, -0.6536,  0.1415, -0.9899,  0.7785, -0.6277,  0.9933, -0.1157,\n",
      "          0.9536,  0.3011]])\n",
      "==================================================\n",
      "Embedding + 位置编码 结果验证\n",
      "==================================================\n",
      "原始Embedding形状：torch.Size([11, 10, 64])\n",
      "添加位置编码后形状：torch.Size([11, 10, 64])\n",
      "\n",
      "第一个句子前2个token的最终表示（前10维）：\n",
      "token1（清晨）： tensor([-0.2951,  0.2095,  0.0197,  1.3987,  0.4551,  1.2550,  1.2473,  1.1035,\n",
      "        -1.2611,  1.2596], grad_fn=<SliceBackward0>)\n",
      "token2（阳光）： tensor([ 0.7890,  1.8493,  0.5053,  0.0667,  1.7511, -0.6152, -0.3826, -0.2176,\n",
      "        -0.4186,  0.4364], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.单头注意力",
   "id": "3d64aab2323b762a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.780563Z",
     "start_time": "2025-10-30T02:51:34.758492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2. Scaled Dot-Product Attention（单头）\n",
    "# ------------------------------------------------------------------\n",
    "def attention(query, key, value,\n",
    "              mask: Optional[torch.Tensor] = None,\n",
    "              dropout: Optional[nn.Module] = None):\n",
    "    \"\"\"\n",
    "    单头缩放点积注意力\n",
    "    Args:\n",
    "        query/key/value: [batch, heads, seq_q/k/v, head_dim]\n",
    "        mask: 任意可 broadcast 到 [..., seq_q, seq_k] 的 0/1 张量，0 表示遮盖\n",
    "    Returns:\n",
    "        out: [..., seq_q, head_dim]\n",
    "        attn_weights: [..., seq_q, seq_k]\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)                                 # head_dim\n",
    "    # 1) 点积 + 缩放\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))  # [..., seq_q, seq_k]\n",
    "    scores = scores / math.sqrt(d_k)                     # 缩放防止梯度消失\n",
    "\n",
    "    # 2) mask：把填充位置变成 -inf，softmax 后概率≈0\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # 3) softmax 得到注意力权重\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 4) dropout（可选）\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 5) 加权求和得到输出\n",
    "    out = torch.matmul(p_attn, value)                    # [..., seq_q, head_dim]\n",
    "    return out, p_attn"
   ],
   "id": "9c380738824d03ec",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "测试用例",
   "id": "5a48673d0a15eb0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.810962Z",
     "start_time": "2025-10-30T02:51:34.792568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 64          # 与 Embedding 维度一致\n",
    "head_dim = 64         # 单头\n",
    "assert d_model % head_dim == 0\n",
    "n_heads = d_model // head_dim   # 1\n",
    "\n",
    "# 一个线性层即可，因为单头\n",
    "W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "print(\"W_q：\\n\", W_q)\n",
    "W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "# 初始化（可选）\n",
    "nn.init.xavier_uniform_(W_q.weight)\n",
    "nn.init.xavier_uniform_(W_k.weight)\n",
    "nn.init.xavier_uniform_(W_v.weight)\n",
    "\n",
    "# 假设 chinese_embedded_output 是你「Embedding + 位置编码」后的张量，形状 [11, 10, 64]\n",
    "Q = W_q(chinese_embedded_output)   # [11, 10, 64]\n",
    "K = W_k(chinese_embedded_output)   # [11, 10, 64]\n",
    "V = W_v(chinese_embedded_output)   # [11, 10, 64]\n",
    "\n",
    "# 为了复用你之前写的 attention()，需要把 heads 维显式拆出来\n",
    "# 单头场景：直接 unsqueeze(1) 即可\n",
    "Q = Q.unsqueeze(1)   # [11, 1, 10, 64]\n",
    "K = K.unsqueeze(1)\n",
    "V = V.unsqueeze(1)\n",
    "\n",
    "# 如果序列长度一致且没有 padding，mask 可以不给\n",
    "out, attn_weights = attention(Q, K, V)   # out: [11, 1, 10, 64]\n",
    "\n",
    "# 把 heads 维合并回去\n",
    "out = out.squeeze(1)                     # [11, 10, 64]\n",
    "\n",
    "# 打印张量维度，用中文一看就懂\n",
    "print(f\"Q 的维度：{Q.shape}\")            # Q 的维度：torch.Size([11, 1, 10, 64])\n",
    "print(f\"注意力输出维度：{out.shape}\")     # 注意力输出维度：torch.Size([11, 10, 64])\n",
    "print(f\"注意力权重维度：{attn_weights.shape}\")  # 注意力权重维度：torch.Size([11, 1, 10, 10])"
   ],
   "id": "76a37f2ec635c88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_q：\n",
      " Linear(in_features=64, out_features=64, bias=False)\n",
      "Q 的维度：torch.Size([11, 1, 10, 64])\n",
      "注意力输出维度：torch.Size([11, 10, 64])\n",
      "注意力权重维度：torch.Size([11, 1, 10, 10])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.多头注意力",
   "id": "ffe373a353108f05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.857147Z",
     "start_time": "2025-10-30T02:51:34.833083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 3. Multi-Head Attention\n",
    "# ------------------------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads: int, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads   # 单头维度\n",
    "        self.heads = heads\n",
    "        # 3 个线性映射：把 Q/K/V 从 d_model -> d_model，再拆成 heads 个头\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, d_model)  # 最后 concat 后再投影\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # query/key/value: [batch, seq_len, d_model]\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 辅助函数：reshape + transpose 得到 [batch, heads, seq, d_k]\n",
    "        def shape(x):\n",
    "            return x.view(batch_size, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 1) 线性投影 + 拆头\n",
    "        q = shape(self.linear_q(query))\n",
    "        k = shape(self.linear_k(key))\n",
    "        v = shape(self.linear_v(value))\n",
    "\n",
    "        # 2) mask 加一维适配 heads\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)   # [batch, 1, 1, seq_k]\n",
    "\n",
    "        # 3) 缩放点积注意力\n",
    "        x, attn = attention(q, k, v, mask=mask, dropout=self.attn_dropout)\n",
    "        # x: [batch, heads, seq_q, d_k]\n",
    "\n",
    "        # 4) 合并 heads\n",
    "        x = (x.transpose(1, 2)                       # [batch, seq_q, heads, d_k]\n",
    "             .contiguous()\n",
    "             .view(batch_size, -1, self.heads * self.d_k))  # [batch, seq_q, d_model]\n",
    "\n",
    "        # 5) 输出投影\n",
    "        return self.linear_out(x)"
   ],
   "id": "c8d31cecb1b71292",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.前馈神经网络",
   "id": "76d863f625939e4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.873151Z",
     "start_time": "2025-10-30T02:51:34.865147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 4. Position-wise Feed-Forward Network\n",
    "# ------------------------------------------------------------------\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    两层全连接+ReLU+Dropout，同一序列位置共享参数\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(d_model, d_ff)\n",
    "        self.w2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq, d_model]\n",
    "        return self.w2(self.dropout(F.relu(self.w1(x))))   # 先升维再降维"
   ],
   "id": "43b49f32fe30b40",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.编码层",
   "id": "a3e578b32f82bb5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.889150Z",
     "start_time": "2025-10-30T02:51:34.882152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 5. Encoder Layer\n",
    "# ------------------------------------------------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    每个 EncoderLayer = 多头自注意力 + Add&Norm + FFN + Add&Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        # 1) 自注意力\n",
    "        attn_out = self.self_attn(x, x, x, mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))   # 残差+LayerNorm\n",
    "\n",
    "        # 2) FFN\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "        return x"
   ],
   "id": "90f0ca025e1acb66",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7.解码层",
   "id": "9cd800a40d6e46aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.920148Z",
     "start_time": "2025-10-30T02:51:34.898151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 6. Decoder Layer\n",
    "# ------------------------------------------------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    每个 DecoderLayer = Masked 自注意力 + 交叉注意力 + FFN\n",
    "    每一步都残差+LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(heads, d_model, dropout)\n",
    "        self.src_attn = MultiHeadAttention(heads, d_model, dropout)  # 交叉注意力\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        # 1) Masked 自注意力（防止看到未来）\n",
    "        attn1 = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn1))\n",
    "\n",
    "        # 2) 交叉注意力：Query=解码器，Key/Value=编码器输出\n",
    "        attn2 = self.src_attn(x, memory, memory, mask=memory_mask)\n",
    "        x = self.norm2(x + self.dropout(attn2))\n",
    "\n",
    "        # 3) FFN\n",
    "        ff = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff))\n",
    "        return x"
   ],
   "id": "1c2ca0114a536d4b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "8.编码解码堆叠",
   "id": "8278b3f1870852c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.936149Z",
     "start_time": "2025-10-30T02:51:34.927148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 7. Encoder / Decoder 堆叠\n",
    "# ------------------------------------------------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        # 深拷贝 N 个 EncoderLayer\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        # 最后再加一次 LayerNorm（与原始论文一致）\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.linear_out.out_features)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=src_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(layer.self_attn.linear_out.out_features)\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask=None, memory_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        return self.norm(x)"
   ],
   "id": "a96589eb1baab846",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "9.完整的Transfomer",
   "id": "41e160f0ef17954"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.966966Z",
     "start_time": "2025-10-30T02:51:34.945450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 8. 完整 Transformer\n",
    "# ------------------------------------------------------------------\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, tgt_vocab,\n",
    "                 d_model=512, N=6, heads=8, d_ff=2048,\n",
    "                 dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        # 源/目标嵌入 + 位置编码\n",
    "        self.src_embed = nn.Sequential(\n",
    "            nn.Embedding(src_vocab, d_model),\n",
    "            PositionalEncoding(d_model, max_len)\n",
    "        )\n",
    "        self.tgt_embed = nn.Sequential(\n",
    "            nn.Embedding(tgt_vocab, d_model),\n",
    "            PositionalEncoding(d_model, max_len)\n",
    "        )\n",
    "\n",
    "        # 编码器 & 解码器\n",
    "        encoder_layer = EncoderLayer(d_model, heads, d_ff, dropout)\n",
    "        decoder_layer = DecoderLayer(d_model, heads, d_ff, dropout)\n",
    "        self.encoder = Encoder(encoder_layer, N)\n",
    "        self.decoder = Decoder(decoder_layer, N)\n",
    "\n",
    "        # 输出 softmax 前的线性层\n",
    "        self.out = nn.Linear(d_model, tgt_vocab)\n",
    "\n",
    "        # Xavier 均匀初始化（>1 维参数）\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # 编码阶段：src -> memory\n",
    "    def encode(self, src, src_mask=None):\n",
    "        return self.encoder(self.src_embed(src), src_mask=src_mask)\n",
    "\n",
    "    # 解码阶段：tgt + memory -> 隐状态\n",
    "    def decode(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory,\n",
    "                            tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "\n",
    "    # 完整前向：src+tgt -> logits\n",
    "    def forward(self, src, tgt,\n",
    "                src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        memory = self.encode(src, src_mask=src_mask)\n",
    "        dec = self.decode(tgt, memory,\n",
    "                          tgt_mask=tgt_mask, memory_mask=memory_mask)\n",
    "        return self.out(dec)          # [batch, tgt_len, tgt_vocab]"
   ],
   "id": "920994b336cdefa7",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "10.掩码",
   "id": "945dfbe336c44e52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T02:51:34.982521Z",
     "start_time": "2025-10-30T02:51:34.974522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 9. Mask 工具函数\n",
    "# ------------------------------------------------------------------\n",
    "def make_src_mask(src, pad_idx=0):\n",
    "    \"\"\"\n",
    "    屏蔽 <PAD> 位置，返回 [batch, 1, src_len]\n",
    "    在多头注意力里会自动广播到 [batch, heads, src_len, src_len]\n",
    "    \"\"\"\n",
    "    mask = (src != pad_idx).unsqueeze(-2)   # [batch, 1, src_len]\n",
    "    return mask\n",
    "\n",
    "def make_tgt_mask(tgt, pad_idx=0):\n",
    "    \"\"\"\n",
    "    结合：\n",
    "      1) padding mask：屏蔽 <PAD>\n",
    "      2) subsequent mask：屏蔽未来信息（上三角）\n",
    "    返回 [batch, 1, tgt_len, tgt_len]\n",
    "    \"\"\"\n",
    "    batch_size, tgt_len = tgt.size()\n",
    "    # padding mask\n",
    "    pad_mask = (tgt != pad_idx).unsqueeze(-2)        # [batch, 1, tgt_len]\n",
    "    # subsequent mask（下三角为 1）\n",
    "    subsequent = torch.triu(\n",
    "        torch.ones((1, tgt_len, tgt_len), device=tgt.device),\n",
    "        diagonal=1\n",
    "    ).bool()                                           # 上三角 True\n",
    "    subsequent_mask = ~subsequent                      # 下三角 True\n",
    "\n",
    "    # 合并：既要非 PAD，也要非未来\n",
    "    mask = pad_mask & subsequent_mask                  # 广播到 [batch, 1, tgt_len, tgt_len]\n",
    "    return mask"
   ],
   "id": "4f0a5dd784f13819",
   "outputs": [],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
